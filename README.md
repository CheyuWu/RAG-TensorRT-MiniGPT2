# RAG-TensorRT-MiniGPT2

ðŸš€ A Retrieval-Augmented Generation (RAG) pipeline built with **PyTorch** and accelerated using **NVIDIA TensorRT**.  
This project demonstrates how to combine **custom Transformer (GPT2-style) layers** with **TensorRT inference optimization**,  
providing both functional correctness and performance benchmarks.

## âœ¨ Features
- ðŸ”Ž **Retrieval-Augmented Generation (RAG)** with FAISS vector store  
- ðŸ§  **Custom GPT2-style Transformer block** implemented in PyTorch  
- ðŸ”„ **ONNX export** and **TensorRT engine building** (FP32/FP16)  
- âš¡ **Performance evaluation** comparing PyTorch vs TensorRT inference (latency, throughput, memory)  
- ðŸ“Š End-to-end demo: query â†’ retrieval â†’ model inference â†’ answer  

## ðŸ“Œ Motivation
Modern LLM-based applications require not only accurate answers (via RAG),  
but also **low-latency and efficient inference**.  
This project explores how **TensorRT optimizations** can significantly improve inference performance for Transformer-based models.

